{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 01\n",
    "\n",
    "Content:\n",
    "1. Softmax\n",
    "2. Byte-Pair\n",
    "3. Tokenizer\n",
    "4. NLP-Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "Rem: you get bonus for this exercise if you answer at least 3 out of 4 questions.\n",
    "\n",
    "Here we are going to answer the question, why softmax is called \"softmax\" and investigate softmax characteristics. The $i \\text{-th}$ component of the softmax is given by:\n",
    "\n",
    "$$f(x_i) := \\text{softmax}(x_i) = \\frac{\\exp x_i}{ \\sum_{j = 1}^k \\exp x_j}$$\n",
    "\n",
    "\n",
    "And thus the vector is $f(\\mathbf x) = [f(x_1), \\dots, f(x_k)]$ where $\\mathbf x = (x_1, \\dots, x_k)$.\n",
    "\n",
    "\n",
    "1. show that $f(\\mathbf x)$ can be interpreted as a probability. To do so, show that $\\sum_{i=1}^k f(x_i) = 1$ and $f(x) > 0 \\quad \\forall x \\in \\mathbb{R}$\n",
    "\n",
    "> ![](aufgabe_1-1.png)\n",
    "\n",
    "2. show that softmax is $C^\\infty$, i.e. that you can calculate the derivative of $f(x_i)$ as often as you want with respect to $x_i$. You can use the fact that $\\exp x$ is smooth (smooth means $C ^ \\infty$)\n",
    "\n",
    "> ![](aufgabe_1-2.png)\n",
    "\n",
    "3. show that if $x_i \\lt x_j$ then $f(x_i) \\lt f(x_j)$. Does the converse hold as well?\n",
    "\n",
    "> ![](aufgabe_1-3.png)\n",
    "\n",
    "4. what is the limit of $f(x_i)$ for $x_i \\to + \\infty$? Same question for $x \\to - \\infty$.\n",
    "\n",
    "> ![](aufgabe_1-4.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-pair encoding\n",
    "\n",
    "Implement Byte-paid encoding and reproduce the example in chapter 2.5.2 of [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"Byte Pair Encoding\"](byte_pair_encoding.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Vocabulary: {'l', 'wi', 'new', 'd', 'n', 'r', 'lowe', 'lo', 's', 'w', 'i', 'e', 't', 'low', 'est', 'ne', 'es', 'newest', 'o', 'lower'}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def getTokenDistribution(corpus):\n",
    "\n",
    "    # find the most frequent pair of adjacent tokens in the corpus\n",
    "    pairs = Counter()\n",
    "    for word in corpus:\n",
    "        symbols = word.split()\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += 1\n",
    "\n",
    "    # return a dictionary with the pair as key and the frequency as value\n",
    "    return pairs\n",
    "\n",
    "def mergeVocabulary(pair, corpus):\n",
    "    \n",
    "    newCorpus = []\n",
    "    \n",
    "    # make new token by concatenating\n",
    "    bigram = ' '.join(pair)\n",
    "    for word in corpus:\n",
    "        # update the corpus\n",
    "        newWord = word.replace(bigram, ''.join(pair))\n",
    "        newCorpus.append(newWord)\n",
    "\n",
    "    return newCorpus\n",
    "\n",
    "def bytePairEncoding(corpus, numberOfMerges):\n",
    "    \n",
    "    # all unique characters in the corpus\n",
    "    # initial set of tokens is characters\n",
    "    vocabulary = set(char for word in corpus for char in word)\n",
    "    corpus = [' '.join(word) for word in corpus]\n",
    "\n",
    "    # merge tokens k times\n",
    "    for _ in range(numberOfMerges):\n",
    "        pairs = getTokenDistribution(corpus)\n",
    "        if not pairs:\n",
    "            break\n",
    "        # most frequent pair of adjacent tokens in the corpus\n",
    "        mostFrequentPair = max(pairs, key=pairs.get)\n",
    "\n",
    "        # make new token by concatenating and update the corpus\n",
    "        corpus = mergeVocabulary(mostFrequentPair, corpus)\n",
    "\n",
    "        # update the vocabulary\n",
    "        vocabulary.add(''.join(mostFrequentPair))\n",
    "\n",
    "    return vocabulary\n",
    "\n",
    "corpus = [\"low\", \"lower\", \"newest\", \"widest\"]\n",
    "vocab = bytePairEncoding(corpus, numberOfMerges=10)\n",
    "print(\"Final Vocabulary:\", vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "Go through the HuggingFace tutorial on [tokenizers](https://huggingface.co/learn/nlp-course/chapter2/4) and answer:\n",
    "\n",
    "- why do we need tokenizers?\n",
    "\n",
    "> Tokenizers sind notwendig, weil LLMs nicht mit Wörter umgehen kann, sondern eine numerische Repräsentation eines Worts benötigt.\n",
    "\n",
    "- what is the difference between a character-based, word-based and a subword-based tokenizer? What are the advantages and disadvantages of each?\n",
    "\n",
    "> ## Word-based Tokenizer\n",
    "> * Tokens werden aufgrund der Wortgrenzen erstellt.\n",
    "> * Wörter, die nicht bekannt sind, werden mit einem Unknown-Token (bspw. <UNK>) markiert\n",
    "> * Jedes Wort bekommt eine eigene ID\n",
    "> \n",
    "> ### Vorteile\n",
    "> * Simpel\n",
    "> \n",
    "> ### Nachteile\n",
    "> * Verwandte Worte erhalten komplett unterschiedliche Ids\n",
    "> * Grosses Vokabular\n",
    "> * Unknown-Tokens bedeuten für das Modell dasselbe, obschon der dahinterstehende Text unterschiedliche Bedeutung haben kann\n",
    ">\n",
    "> ## Character-based Tokenizer\n",
    "> * Text wird in die einzelnen Zeichen unterteilt --> Token ist ein Character\n",
    "> \n",
    "> ### Vorteile\n",
    "> * Kleineres Vokabular\n",
    "> * Weniger Unknown-Tokens\n",
    "> \n",
    "> ### Nachteile\n",
    "> * Grosse Anzahl Token --> Modell muss mehr Tokens verarbeiten\n",
    "> * Ein Token hat eine kleinere Bedeutung als beim Word-Based Tokenizer\n",
    ">\n",
    "> ## Subword-based Tokenizer\n",
    "> * Kombination von Word-Based und Character-Based Tokenizer\n",
    "> * Häufige Wörter sollen nicht in Subwords unterteilt werden (dog --> dog)\n",
    "> * Seltene Wörter sollen in sinnvolle Subwords unterteilt werden (dogs --> dog, s)\n",
    "> \n",
    "> ### Vorteile\n",
    "> * Gleichgewicht zwischen Vokabulargrösse und Ausdrucksstärke\n",
    "> * Weniger Unknown-Tokens, da unbekannte Wörter in bekannte Subwörter zerlegt werden können\n",
    "> * Verwandte Wörter teilen oft gemeinsame Subwörter\n",
    "> \n",
    "> ### Nachteile\n",
    "> * Komplexere Tokenisierung\n",
    "> * Nicht alle Subwords entsprechen sinnvollen sprachlichen Einheiten\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline\n",
    "\n",
    "Recall the NLP-Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{mermaid}\n",
    "%%| echo: false\n",
    "flowchart TD\n",
    "    A[Data Acquistion] --> B[Preprocessing and Normalization]\n",
    "    B --> C[Modelling]\n",
    "    C --> C\n",
    "    C --> D[Model evaluation]\n",
    "    D --> |more preprocessing needed| B\n",
    "    D --> |more/different data needed| A\n",
    "    D --> E[Added Value]\n",
    "    E --> |reiterate| A\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to do some prepocessing and normalization steps. Your task is to do:\n",
    "\n",
    "\n",
    "\n",
    "1. **Data Collection**: Collect a corpus of text data. It is completely up to you.\n",
    "2. **Data Cleaning**: Clean the collected data by removing any irrelevant information such as HTML tags, URLs, numbers, etc. This step depends in the corpus you chose:\n",
    "    - count the vocabulary size before and after cleaning\n",
    "3. **Tokenization**: Apply a tokenizer (e.g. using https://www.nltk.org/):\n",
    "    - count the vocabulary size before and after tokenization\n",
    "    - how much time is needed per word in average?\n",
    "4. **Stopwords Removal**: Identify and remove stopwords from the tokens. Stopwords are common words that do not contribute much to the meaning of a sentence, such as 'the', 'is', 'in', etc.\n",
    "    - count the vocabulary size before and after stopword removal\n",
    "5. **Stemming and Lemmatization**: Apply stemming and lemmatization techniques to the tokens and observe the differences. Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form. Lemmatization, on the other hand, reduces words to their base word, which is linguistically correct lemmas. It transforms root word with the use of vocabulary and morphological analysis.\n",
    "    - count the vocabulary size for the stemmed and lemmatized vocabulary\n",
    "6. **Compare**: \n",
    "    - create a barplot of the results you have collected above (https://plotly.com/python/bar-charts/)\n",
    "\n",
    "Useful libraries:\n",
    "\n",
    "- NLTK, Spacy\n",
    "- plotly or matplotlib for plotting graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\dario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "\n",
    "raw_text = gutenberg.raw('shakespeare-hamlet.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size before cleaning: 7422\n",
      "Vocabulary size after cleaning: 5430\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned = re.sub(r\"<.*?>\", \" \", text)  # remove HTML tags\n",
    "    cleaned = re.sub(r\"http\\S+|www\\S+\", \" \", cleaned)  # remove URLs\n",
    "    cleaned = re.sub(r\"[^A-Za-z\\s]\", \" \", cleaned)  # remove numbers and punctuation\n",
    "    cleaned = re.sub(r\"\\s+\", \" \", cleaned).strip()  # remove extra spaces\n",
    "    return cleaned\n",
    "\n",
    "cleaned_text = clean_text(raw_text)\n",
    "\n",
    "vocab_before = len(set(raw_text.split()))\n",
    "vocab_after = len(set(cleaned_text.split()))\n",
    "\n",
    "print(\"Vocabulary size before cleaning:\", vocab_before)\n",
    "print(\"Vocabulary size after cleaning:\", vocab_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 5428\n",
      "Average time per word: 1.9758370121523408e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\dario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "start = time.time()\n",
    "tokens = word_tokenize(cleaned_text)\n",
    "end = time.time()\n",
    "\n",
    "vocab_tokenized = len(set(tokens))\n",
    "avg_time_per_word = (end - start) / len(tokens)\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_tokenized)\n",
    "print(\"Average time per word:\", avg_time_per_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size without stop words: 5230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens_no_stop = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "vocab_no_stop = len(set(tokens_no_stop))\n",
    "\n",
    "print(\"Vocabulary size without stop words:\", vocab_no_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\dario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\dario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size after stemming: 3649\n",
      "Vocabulary size after lemmatization: 4322\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stemmed = [stemmer.stem(word) for word in tokens_no_stop]\n",
    "lemmatized = [lemmatizer.lemmatize(word.lower()) for word in tokens_no_stop]\n",
    "\n",
    "vocab_stemmed = len(set(stemmed))\n",
    "vocab_lemmatized = len(set(lemmatized))\n",
    "\n",
    "print(\"Vocabulary size after stemming:\", vocab_stemmed)\n",
    "print(\"Vocabulary size after lemmatization:\", vocab_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "type": "bar",
         "x": [
          "Original Raw",
          "Cleaned",
          "Tokenized",
          "No Stopwords",
          "Stemmed",
          "Lemmatized"
         ],
         "y": [
          7422,
          5430,
          5428,
          5230,
          3649,
          4322
         ]
        }
       ],
       "layout": {
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermap": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermap"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Vocabulary Size Across Preprocessing Stages"
        },
        "xaxis": {
         "title": {
          "text": "Stage"
         }
        },
        "yaxis": {
         "title": {
          "text": "Vocabulary Size"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "labels = [\n",
    "    \"Original Raw\",\n",
    "    \"Cleaned\",\n",
    "    \"Tokenized\",\n",
    "    \"No Stopwords\",\n",
    "    \"Stemmed\",\n",
    "    \"Lemmatized\"\n",
    "]\n",
    "values = [\n",
    "    vocab_before,\n",
    "    vocab_after,\n",
    "    vocab_tokenized,\n",
    "    vocab_no_stop,\n",
    "    vocab_stemmed,\n",
    "    vocab_lemmatized\n",
    "]\n",
    "\n",
    "fig = go.Figure([go.Bar(x=labels, y=values)])\n",
    "fig.update_layout(title=\"Vocabulary Size Across Preprocessing Stages\",\n",
    "                  xaxis_title=\"Stage\", yaxis_title=\"Vocabulary Size\")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
